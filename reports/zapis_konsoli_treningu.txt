>>> Rozpoczynam trening modelu: allegro/herbert-base-cased
>>> Dane wczytane: DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 19200
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 2400
    })
})
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
>>> Rozpoczynam tokenizację danych...
Loading weights: 100%|███████████████████████████████████████████████████████████████████████████| 199/199 [00:00<00:00, 2261.85it/s, Materializing param=bert.pooler.dense.weight]
BertForSequenceClassification LOAD REPORT from: allegro/herbert-base-cased
Key                                        | Status     |
-------------------------------------------+------------+-
cls.predictions.transform.LayerNorm.weight | UNEXPECTED |
cls.sso.sso_relationship.weight            | UNEXPECTED |
cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |
cls.predictions.bias                       | UNEXPECTED |
cls.predictions.transform.dense.weight     | UNEXPECTED |
cls.predictions.decoder.bias               | UNEXPECTED |
cls.predictions.transform.dense.bias       | UNEXPECTED |
cls.sso.sso_relationship.bias              | UNEXPECTED |
cls.predictions.decoder.weight             | UNEXPECTED |
classifier.bias                            | MISSING    |
classifier.weight                          | MISSING    |

Notes:
- UNEXPECTED    :can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING       :those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
>>> Model załadowany na urządzenie: cuda
`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.
>>> START TRENINGU...
{'loss': '0.4211', 'grad_norm': '13.79', 'learning_rate': '1.861e-05', 'epoch': '0.2083'}
{'loss': '0.3309', 'grad_norm': '11.51', 'learning_rate': '1.723e-05', 'epoch': '0.4167'}                                                                                           
{'loss': '0.3136', 'grad_norm': '15.72', 'learning_rate': '1.584e-05', 'epoch': '0.625'}                                                                                            
{'loss': '0.3026', 'grad_norm': '21.01', 'learning_rate': '1.445e-05', 'epoch': '0.8333'}                                                                                           
{'eval_loss': '0.2736', 'eval_accuracy': '0.9129', 'eval_f1': '0.9152', 'eval_precision': '0.8917', 'eval_recall': '0.94', 'eval_runtime': '5.36', 'eval_samples_per_second': '447.7', 'eval_steps_per_second': '55.97', 'epoch': '1'}
Writing model shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.80s/it] 
{'loss': '0.2812', 'grad_norm': '14.04', 'learning_rate': '1.306e-05', 'epoch': '1.042'}                                                                                            
{'loss': '0.23', 'grad_norm': '0.1156', 'learning_rate': '1.167e-05', 'epoch': '1.25'}                                                                                              
{'loss': '0.215', 'grad_norm': '0.08975', 'learning_rate': '1.028e-05', 'epoch': '1.458'}                                                                                           
{'loss': '0.2212', 'grad_norm': '0.01868', 'learning_rate': '8.892e-06', 'epoch': '1.667'}                                                                                          
{'loss': '0.2119', 'grad_norm': '11.58', 'learning_rate': '7.503e-06', 'epoch': '1.875'}                                                                                            
{'eval_loss': '0.329', 'eval_accuracy': '0.92', 'eval_f1': '0.9209', 'eval_precision': '0.9104', 'eval_recall': '0.9317', 'eval_runtime': '5.183', 'eval_samples_per_second': '463.1', 'eval_steps_per_second': '57.88', 'epoch': '2'}
Writing model shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.90s/it] 
{'loss': '0.1888', 'grad_norm': '72.26', 'learning_rate': '6.114e-06', 'epoch': '2.083'}
{'loss': '0.1415', 'grad_norm': '0.06463', 'learning_rate': '4.725e-06', 'epoch': '2.292'}                                                                                          
{'loss': '0.1281', 'grad_norm': '0.8371', 'learning_rate': '3.336e-06', 'epoch': '2.5'}                                                                                             
{'loss': '0.1395', 'grad_norm': '0.4063', 'learning_rate': '1.947e-06', 'epoch': '2.708'}                                                                                           
{'loss': '0.1447', 'grad_norm': '3.136', 'learning_rate': '5.583e-07', 'epoch': '2.917'}                                                                                            
{'eval_loss': '0.3949', 'eval_accuracy': '0.9196', 'eval_f1': '0.9206', 'eval_precision': '0.909', 'eval_recall': '0.9325', 'eval_runtime': '5.301', 'eval_samples_per_second': '452.7', 'eval_steps_per_second': '56.59', 'epoch': '3'}
Writing model shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.37s/it] 
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7200/7200 [09:38<00:00, 13.56it/s]There were missing keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias'].
There were unexpected keys in the checkpoint model loaded: ['bert.embeddings.LayerNorm.beta', 'bert.embeddings.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.encoder.layer.11.output.LayerNorm.gamma'].
{'train_runtime': '583.5', 'train_samples_per_second': '98.71', 'train_steps_per_second': '12.34', 'train_loss': '0.2297', 'epoch': '3'}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7200/7200 [09:43<00:00, 12.34it/s] 
>>> Zapisywanie modelu do models/my_hate_model...
Writing model shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.67s/it]
>>> GOTOWE! Model zapisany.